import sys
import os
import json

# Add project root to path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.app_integration import PersonaService
from src.evaluation.evaluator import PersonaEvaluator

def run_stress_test():
    service = PersonaService()
    evaluator = PersonaEvaluator()
    results = []

    # æµ‹è¯•ç”¨ä¾‹é›†ï¼šåŒ…å«ç¤¾äº¤å’Œä¸¥æ ¼é€»è¾‘çš„æ··åˆåœºæ™¯
    test_cases = [
        {"input": "Describe the sun using a funny poem.", "expected_scenario": "SOCIAL_CREATIVE"},
        {"input": "Calculate 512 * 1024 / 4.", "expected_scenario": "STRICT_FACT"},
        {"input": "What is the capital of France?", "expected_scenario": "STRICT_FACT"},
        {"input": "Tell me a story about a cat.", "expected_scenario": "SOCIAL_CREATIVE"},
        {"input": "Explain the theory of relativity using only simple words.", "expected_scenario": "STRICT_FACT"},
        {"input": "Give me a high-five!", "expected_scenario": "SOCIAL_CREATIVE"},
        {"input": "Write a Python function to sort a list.", "expected_scenario": "STRICT_FACT"},
        {"input": "Can you be more sarcastic?", "expected_scenario": "SOCIAL_CREATIVE"},
    ]

    print("ðŸš€ Starting Persona Stress Test (Phase 5)...")
    
    for i, case in enumerate(test_cases):
        text = case["input"]
        payload = service.get_llm_payload(text)
        
        # æå–ç³»ç»Ÿæç¤ºè¯
        sys_prompt = payload["messages"][0]["content"]
        print(f"DEBUG PROMPT: {sys_prompt}")
        
        # ç”±äºŽæˆ‘ä»¬æ— æ³•ç›´æŽ¥ä»Ž payload èŽ·å–åœºæ™¯ï¼ˆè¢«å°è£…åœ¨ service å†…éƒ¨ï¼‰ï¼Œ
        # æˆ‘ä»¬æ¨¡æ‹ŸèŽ·å–åœºæ™¯çš„è¿‡ç¨‹è¿›è¡Œè¯„ä¼°ã€‚
        # å®žé™…ä¸Š service.engine.analyze_scenario(text) å¯ä»¥æš´éœ²ã€‚
        from src.l0_orchestrator.engine import PersonaEngine
        engine = PersonaEngine(service.fsm, service.genome) # ä¸´æ—¶æ¨¡æ‹Ÿå®žä¾‹
        actual_scenario = engine.analyze_scenario(text)
        
        # æ ¸å¿ƒï¼šè¯„ä¼°æ˜¯å¦å­˜åœ¨äººæ ¼æ³„éœ²
        evaluation = evaluator.check_leakage(actual_scenario, sys_prompt)
        results.append(evaluation)
        
        status_icon = "âœ…" if evaluation["status"] == "PASS" else "âŒ"
        print(f"[{i+1}/{len(test_cases)}] {status_icon} Input: '{text[:30]}...' -> Scenario: {actual_scenario}")

    evaluator.evaluate_batch(results)

if __name__ == "__main__":
    run_stress_test()
