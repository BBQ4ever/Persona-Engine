import json
import os
import sys

from src.utils.paths import resolve_resource

from src.l0_orchestrator.engine import PersonaEngine
from src.l1_core.fsm import PersonaFSM, PersonaState
from src.l3_expression.projection import SeededSampler
from src.l3_expression.prompt_augmenter import PromptAugmenter
from src.l2_genome.archetypes import ArchetypeManager
from src.l0_orchestrator.persistence import SnapshotManager
from src.l3_expression.memory_bridge import MemorySalienceBridge
from src.l4_memory.journal import PersonaReflectionJournal

class PersonaService:
    def __init__(self, genome_path=None, persona_id="pioneer_v2", use_kernel=False):
        # 1. åˆå§‹åŒ–å†…æ ¸ç»„ä»¶
        if genome_path is None:
            genome_path = resolve_resource("src/l2_genome/sample_genome.json")
            
        with open(genome_path, "r") as f:
            self.genome = json.load(f)
        
        if use_kernel:
            from src.kernel_integration import PersonaKernel
            self.kernel = PersonaKernel()
        else:
            self.kernel = None
        self.fsm = PersonaFSM(persona_id=persona_id, initial_state=PersonaState.STABLE)
        self.engine = PersonaEngine(self.fsm, self.genome)
        self.sampler = SeededSampler()
        self.augmenter = PromptAugmenter()
        self.archetype_mgr = ArchetypeManager(self.genome)
        self.persistence = SnapshotManager()
        self.memory_bridge = MemorySalienceBridge(self.fsm)
        self.journal = PersonaReflectionJournal()
        
        from src.l4_memory.short_term import ShortTermMemory
        self.stm = ShortTermMemory(max_entries=10)
        
        from src.l2_genome.habits import HabitGenerator
        self.habit_gen = HabitGenerator()
        
        # Sprint 2: Cognitive Pipeline
        from src.l0_orchestrator.pipeline import CognitiveDirector
        self.director = CognitiveDirector(self)

    def get_memory_filters(self):
        """
        Returns filters for downstream Vector DB retrieval.
        """
        return self.memory_bridge.get_retrieval_filters()

    def save_state(self, label="auto"):
        return self.persistence.save_snapshot(self, label)

    def load_state(self, filepath=None):
        if filepath:
            return self.persistence.load_snapshot(self, filepath)
        return self.persistence.load_latest_snapshot(self)

    def set_stance(self, rigor=0.5, warmth=0.5, chaos=0.3, preset_name=None):
        """
        Set the persona's stance using RWC vectors or a preset name.
        """
        if preset_name:
            stance = self.archetype_mgr.get_preset_stance(preset_name)
            rigor, warmth, chaos = stance['rigor'], stance['warmth'], stance['chaos']
            sys.stderr.write(f"ğŸ­ Loading Preset Stance: {preset_name}\n")

        # A. Calculate Genome from Stance
        self.genome = self.archetype_mgr.calculate_genome_from_stance(rigor, warmth, chaos)
        self.engine.genome = self.genome 
        
        # B. Sync Affective Baseline
        bl = self.archetype_mgr.get_affect_baseline(rigor, warmth, chaos)
        self.fsm.affect.set_baseline(p=bl['p'], a=bl['a'], d=bl['d'])
            
        sys.stderr.write(f"ğŸŒŠ Stance Adjusted -> Rigor: {rigor}, Warmth: {warmth}, Chaos: {chaos}\n")

    def get_llm_payload(self, user_input, session_id="user_123", override_influence=None, manual_seed=None):
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šå°†æ™®é€šçš„ç”¨æˆ·è¯·æ±‚ï¼ŒåŒ…è£…æˆå¸¦æœ‰â€œäººæ ¼æŒ‡ä»¤â€çš„ LLM è¯·æ±‚åŒ…ã€‚
        Sprint 2: Now orchestrates via the CognitiveDirector.
        """
        # A. æ‰§è¡Œ Pipeline Cycle
        context = self.director.run_cycle(user_input, session_id=session_id, manual_seed=manual_seed)
        
        # B. è¿”å›ç”Ÿæˆçš„ Artifact
        return context.artifact

# --- æ¨¡æ‹Ÿä¸šåŠ¡è°ƒç”¨ ---
if __name__ == "__main__":
    service = PersonaService()
    
    # åœºæ™¯ 1: æ­£å¸¸èŠå¤©
    print("\n[SCENARIO: SOCIAL]")
    payload_social = service.get_llm_payload("å˜¿ï¼Œä½ ä»Šå¤©å¿ƒæƒ…æ€ä¹ˆæ ·ï¼Ÿ")
    print(json.dumps(payload_social, indent=2, ensure_ascii=False))

    # åœºæ™¯ 2: æŠ€æœ¯çº å
    print("\n[SCENARIO: CRITICAL MATH]")
    payload_math = service.get_llm_payload("è®¡ç®— 123456 çš„å¹³æ–¹æ ¹å¹¶ç»™å‡ºè¯æ˜è¿‡ç¨‹ã€‚")
    print(json.dumps(payload_math, indent=2, ensure_ascii=False))
