import json
import os
import sys

# å°†é¡¹ç›®è·¯å¾„åŠ å…¥ç¯å¢ƒ
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.l0_orchestrator.engine import PersonaEngine
from src.l1_core.fsm import PersonaFSM, PersonaState
from src.l3_expression.projection import SeededSampler
from src.l3_expression.prompt_augmenter import PromptAugmenter
from src.l2_genome.archetypes import ArchetypeManager
from src.l0_orchestrator.persistence import SnapshotManager
from src.l3_expression.memory_bridge import MemorySalienceBridge
from src.l4_memory.journal import PersonaReflectionJournal

class PersonaService:
    """
    è¿™æ˜¯æ‚¨åœ¨ä¸šåŠ¡é€»è¾‘ä¸­ç›´æ¥è°ƒç”¨çš„æœåŠ¡ç±»ã€‚
    """
    def __init__(self, genome_path="src/l2_genome/sample_genome.json"):
        # 1. åˆå§‹åŒ–å†…æ ¸ç»„ä»¶
        with open(genome_path, "r") as f:
            self.genome = json.load(f)
        
        self.fsm = PersonaFSM(persona_id="pioneer_v2", initial_state=PersonaState.STABLE)
        self.engine = PersonaEngine(self.fsm, self.genome)
        self.sampler = SeededSampler()
        self.augmenter = PromptAugmenter()
        self.archetype_mgr = ArchetypeManager(self.genome)
        self.persistence = SnapshotManager()
        self.memory_bridge = MemorySalienceBridge(self.fsm)
        self.journal = PersonaReflectionJournal()

    def get_memory_filters(self):
        """
        Returns filters for downstream Vector DB retrieval.
        """
        return self.memory_bridge.get_retrieval_filters()

    def save_state(self, label="auto"):
        return self.persistence.save_snapshot(self, label)

    def load_state(self, filepath=None):
        if filepath:
            return self.persistence.load_snapshot(self, filepath)
        return self.persistence.load_latest_snapshot(self)

    def set_stance(self, rigor=0.5, warmth=0.5, chaos=0.3, preset_name=None):
        """
        Set the persona's stance using RWC vectors or a preset name.
        """
        if preset_name:
            stance = self.archetype_mgr.get_preset_stance(preset_name)
            rigor, warmth, chaos = stance['rigor'], stance['warmth'], stance['chaos']
            print(f"ğŸ­ Loading Preset Stance: {preset_name}")

        # A. Calculate Genome from Stance
        self.genome = self.archetype_mgr.calculate_genome_from_stance(rigor, warmth, chaos)
        self.engine.genome = self.genome 
        
        # B. Sync Affective Baseline
        bl = self.archetype_mgr.get_affect_baseline(rigor, warmth, chaos)
        self.fsm.affect.set_baseline(p=bl['p'], a=bl['a'], d=bl['d'])
            
        print(f"ğŸŒŠ Stance Adjusted -> Rigor: {rigor}, Warmth: {warmth}, Chaos: {chaos}")

    def get_llm_payload(self, user_input, session_id="user_123", override_influence=None):
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šå°†æ™®é€šçš„ç”¨æˆ·è¯·æ±‚ï¼ŒåŒ…è£…æˆå¸¦æœ‰â€œäººæ ¼æŒ‡ä»¤â€çš„ LLM è¯·æ±‚åŒ…ã€‚
        """
        # A. åœºæ™¯åˆ†æä¸é™çº§ (L0)
        constraints = self.engine.get_effective_constraints(user_input)
        
        # Phase 9: Auto-adjust stance if recommended by engine
        if constraints.get('recommended_stance'):
            s = constraints['recommended_stance']
            self.set_stance(rigor=s['rigor'], warmth=s['warmth'], chaos=s['chaos'])

        # B. è¿™é‡Œçš„é€»è¾‘å°±æ˜¯æ‰§è¡Œé‡‡æ · (L3)
        projection = {}
        target_influence = override_influence if override_influence is not None else constraints['influence']
        
        # Phase 8: Get Affective Warp factors
        affect_warp = self.fsm.affect.get_warp_factors()
        
        for trait in self.genome['loci']:
            val = self.sampler.sample_trait(
                trait, 
                session_id, 
                influence=target_influence,
                affect_warp=affect_warp
            )
            projection[trait['id']] = val
            
        # C. å°†æ•°å€¼æŠ•å½±è½¬åŒ–ä¸ºç³»ç»Ÿæç¤ºè¯ (Prompt Augmenter)
        status = self.fsm.get_status()
        system_instructions = self.augmenter.augment(
            projection, 
            influence=target_influence, 
            intimacy=status['intimacy_level']
        )
        
        # D. æ„é€ æœ€ç»ˆå‘ç»™ LLM çš„æ ¼å¼
        llm_payload = {
            "model": "gpt-4", # æˆ–è€…æ‚¨é€‰æ‹©çš„ä»»ä½•æ¨¡å‹
            "messages": [
                {
                    "role": "system", 
                    "content": f"You are a helpful assistant with the following personality traits:\n{system_instructions}"
                },
                {"role": "user", "content": user_input}
            ]
        }
        
        # Phase 10: Recursive Self-Observation Step
        self.journal.log_entry(status, user_input=user_input)
        
        return llm_payload

# --- æ¨¡æ‹Ÿä¸šåŠ¡è°ƒç”¨ ---
if __name__ == "__main__":
    service = PersonaService()
    
    # åœºæ™¯ 1: æ­£å¸¸èŠå¤©
    print("\n[SCENARIO: SOCIAL]")
    payload_social = service.get_llm_payload("å˜¿ï¼Œä½ ä»Šå¤©å¿ƒæƒ…æ€ä¹ˆæ ·ï¼Ÿ")
    print(json.dumps(payload_social, indent=2, ensure_ascii=False))

    # åœºæ™¯ 2: æŠ€æœ¯çº å
    print("\n[SCENARIO: CRITICAL MATH]")
    payload_math = service.get_llm_payload("è®¡ç®— 123456 çš„å¹³æ–¹æ ¹å¹¶ç»™å‡ºè¯æ˜è¿‡ç¨‹ã€‚")
    print(json.dumps(payload_math, indent=2, ensure_ascii=False))
